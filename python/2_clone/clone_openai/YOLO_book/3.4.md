## 3.4 訓練 YOLO 模型

在本節中，我們將看到如何使用 Darknet 框架來訓練 YOLO 模型來識別物體。

首先，讓我們加載預訓練的模型權重。YOLO v3 官方提供了許多預訓練的模型，其中也包含了對 COCO 數據集的訓練。在這個教程中，我們將使用這些權重，並在其基礎上繼續進行訓練。你可以通過以下命令下載它們：

```
wget https://pjreddie.com/media/files/darknet53.conv.74
```

這將下載一個名為 `darknet53.conv.74` 的文件。我們將使用這個文件來初始化我們的模型權重。

接下來，我們需要準備我們的數據集。我們需要將數據集分為訓練集和驗證集。在這個教程中，我們將使用 PASCAL VOC 2007 數據集。你可以從[這裡](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)下載它。

我們可以使用 `voc_label.py` 腳本來將 PASCAL VOC 轉換為 YOLO 格式。以下是轉換代碼，你可以將它保存到 `voc_label.py` 文件中：

``` python
import xml.etree.ElementTree as ET
import os

classes = ["aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat",
           "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person",
           "pottedplant", "sheep", "sofa", "train", "tvmonitor"]


def convert(size, box):
    dw = 1. / size[0]
    dh = 1. / size[1]
    x = (box[0] + box[1]) / 2.0
    y = (box[2] + box[3]) / 2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x * dw
    w = w * dw
    y = y * dh
    h = h * dh
    return (x, y, w, h)


def convert_annotation(image_id):
    in_file = open('VOCdevkit/VOC2007/Annotations/%s.xml' % image_id)
    out_file = open('VOCdevkit/VOC2007/labels/%s.txt' % image_id, 'w')
    tree = ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)

    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult) == 1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
             float(xmlbox.find('ymax').text))
        bb = convert((w, h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')


wd = os.getcwd()
sets=[('2007', 'train'), ('2007', 'val')]

for year, image_set in sets:
    if not os.path.exists('VOCdevkit/VOC%s/labels/' % year):
        os.makedirs('VOCdevkit/VOC%s/labels/' % year)
    image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)).read().strip().split()
    list_file = open('%s_%s.txt' % (year, image_set), 'w')
    for image_id in image_ids:
        list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\n' % (wd, year, image_id))
        convert_annotation(image_id)
    list_file.close()
```

現在，我們可以使用以下命令來訓練我們的模型：

```
./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74
```

其中 `cfg/voc.data` 是我們數據集的元數據文件，`cfg/yolov3-voc.cfg` 是 YOLO 模型的配置文件。`darknet53.conv.74` 是用於初始化模型權重的文件。

你可以通過修改配置文件來調整模型的超參數。這些超參數包括：模型架構、激活函數、調整項、正則化等。

在訓練時，模型將輸出一些關於訓練進度的信息，例如每個批次的損失值和 batch 的時間等。在訓練完成後，你可以使用以下命令來測試你的模型：

```
./darknet detector test cfg/voc.data cfg/yolov3-voc.cfg <path-to-weight-file> <path-to-image-file>
```

其中 `path-to-weight-file` 是訓練後保存的權重文件路徑，`path-to-image-file` 是需要偵測的圖片路徑。

如果你想要使用 GPU 來進行訓練，你需要確保你的 GPU 可以被 Darknet 認可，並在執行指令時加上 `-gpus` 參數。如果你是使用 CUDNN v5.1 或更高版本，你需要在 Darknet 的配置文件中指定使用 CUDNN，以獲得更好的性能。使用 GPU 訓練模型時，相較使用 CPU，模型訓練速度會顯著提升。